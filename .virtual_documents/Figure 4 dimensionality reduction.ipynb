import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE, MDS
from sklearn.metrics.pairwise import euclidean_distances
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import DBSCAN
import os


####assemble normalised data into data frames

#function to read in modelled decoding time data and normalise to mean
def read_rrt(p,file_name):
    df = pd.read_csv(p + file_name)
    av_dt = np.mean(df['average_decoding_time'])
    if not 'W' in file_name:
        colname = 'base config'
    else:
        colname = file_name[:-4].replace('Wo_','')
    df[colname] = df['average_decoding_time'] / av_dt
    
    return df[['codon',colname]]

#read in filenames
p = 'Data/base_pairing_decoding_times/'
file_names = os.listdir(p)
file_names = [file for file in file_names if file[0] == 'W']
print('Total number of files in folder is ' + str(len(file_names) + 1))

#initiate dataframe with baseline data
norm_df = read_rrt(p,'base_configuration.csv')
for file_name in file_names:
    add_df = read_rrt(p,file_name)
    norm_df = norm_df.merge(add_df,on='codon')

#read in experimental RRT file names
p = 'Data/Experimental data/'
file_names = os.listdir(p)
file_names = [file for file in file_names if file[0] != '.']

#add experimental RRT data (RRTs are already normalised)
for file_name in file_names:
    add_df = pd.read_csv(p + file_name)
    add_df['codon'] = add_df['codon'].str.replace('T','U')
    add_df.columns = [add_df.columns[0],file_name[:-4]]
    norm_df = norm_df.merge(add_df,on='codon')

norm_df = norm_df.set_index('codon')
norm_df = norm_df.iloc[:,0:].T





#explore how much variance is explained when dimensions are reduced using pca

#remove the experimental rrt data for this analysis
pca_df = norm_df
for sample_name in pca_df.index:
    if '20' in sample_name:
        pca_df = pca_df.drop(sample_name,axis=0)

var_ratio = []
for dim in range(norm_df.shape[1]):
  this_pca = PCA(n_components=dim)
  this_pca.fit(norm_df)
  var_ratio.append(np.sum(this_pca.explained_variance_ratio_))


#test which codons contribute most to the information in high dimensional space
test_pca = PCA(n_components=3)
base_pca_result = test_pca.fit(pca_df)
base_evr = np.sum(base_pca_result.explained_variance_ratio_)
codons,d_evrs = [],[]
for codon in pca_df.columns:
    temp_df = pca_df.drop(codon,axis=1)
    test_pca_result = test_pca.fit(temp_df)
    codons.append(codon)
    d_evrs.append(np.sum(base_pca_result.explained_variance_ratio_) - base_evr)

test_results = pd.DataFrame({'codon':codons,'d_evr':d_evrs})
test_results = test_results.sort_values('d_evr',ascending=False)


#exclude data between +- 0.001
x,y = [],[]
for row in test_results.iterrows():
    if row[1]['d_evr'] > 0.001:
        x.append(row[1]['codon'])
        y.append(row[1]['d_evr']*100)

fig,ax = plt.subplots(1,2,figsize = (8,3))
ax[0].scatter(range(pca_df.shape[1]),var_ratio,s=3,c='black')
ax[0].set_xlabel('No of mapped dimensions')
ax[0].set_ylabel('Explained variance (PCA)')
ax[0].set_xlim((-0.5,25.5))
ax[0].set_yticks([0,0.25,0.5,0.75,1])
ax[1].bar(x,y,color='black')
ax[1].set_xlabel('codon')
ax[1].set_ylabel('Effect of codon removal \n on explained variance (%)')
ax[1].tick_params(axis='x', labelrotation=90)
fig.subplots_adjust(wspace=0.3)
#plt.savefig('Figure files/pca analyses.svg')
plt.show()


#print the information conetn for the first n codons
n = 3
print(var_ratio[n])





idces = norm_df.index[1:-5]
unique_elements = []
for idx in idces:
    for el in idx.split(' '):
        if el not in unique_elements:
            unique_elements.append(el)


#define the indices of the literature rrt samples
rrts = list(norm_df.index[-5:])
#prepare a dictionary to hold results
res_dic = {}
#cycle through all unique base pairs:
for e in unique_elements:
    if e not in ['G_','A_']:
        #find all indices that contain this base pair
        has_e = [idx for idx in idces if e in idx]
        delta_dists = []
        #cycle through all indices that have the base pair of interest
        for search_e in has_e:
            #find comparator
            search_set = [search_e_element for search_e_element in search_e.split(' ') if search_e_element != e]
            if len(search_set) == 0:
                comparator = 'base config'
            elif len(search_set) ==1:
                comparator = search_set[0]
            else:
                if ' '.join(search_set) in idces:
                    comparator = ' '.join(search_set)
                else:
                    inv_search_set = [search_set[1],search_set[0]]
                    if ''.join(inv_search_set) in idces:
                        comparator = ''.join(inv_search_set)
            #determine the euclidean distance between search index and literature rrts
            euclid_search_df = norm_df.loc[[search_e] + rrts]
            shift_dist = np.mean(euclidean_distances(euclid_search_df)[0][1:])
            euclid_comp_df = norm_df.loc[[comparator] + rrts]
            comp_dist = np.mean(euclidean_distances(euclid_comp_df)[0][1:])
            delta_dists.append((shift_dist - comp_dist)/ comp_dist)
        res_dic[e] = delta_dists
#clean up dictionary
key_lengths = []
for key in res_dic.keys():
    key_lengths.append(len(res_dic[key]))
max_key_length = max(key_lengths)


for key in res_dic.keys():
    actual_key_length = len(res_dic[key])
    res_dic[key] = res_dic[key] + [np.nan]*(max_key_length-actual_key_length)
res_df = pd.DataFrame(res_dic)
res_df = res_df.reindex(res_df.mean().sort_values().index, axis=1)


fig,ax = plt.subplots(figsize = (8,3))

sns.boxplot(ax=ax,data=res_df,orient='h',color = 'lightgrey',linecolor='black',notch=True,
               flierprops={'marker':'.','ms':3,'alpha':0.5})
ax.set_xticks((-0.05,0,0.05,0.1,0.15,0.2,0.25,0.3,0.35))
ax.plot((0,0),(-0.5,11.5),color='red')
ax.set_xlabel('Change in distance to experimental RRTs')
ax.set_ylabel('Disallowed base pair')
#plt.savefig('Figure files/Base pair movement.svg')
plt.show()



